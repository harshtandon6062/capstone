{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11514d08",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "Make sure MediaPipe is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7b9225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae075c5",
   "metadata": {},
   "source": [
    "## Step 2: Download the Model\n",
    "\n",
    "Download the hand landmarker model if it doesn't exist already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a39d91bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Only download if the model doesn't exist\n",
    "if not os.path.exists('hand_landmarker.task'):\n",
    "    print(\"Downloading model...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        'https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task',\n",
    "        'hand_landmarker.task'\n",
    "    )\n",
    "    print(\"Model downloaded successfully!\")\n",
    "else:\n",
    "    print(\"Model already exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb9146",
   "metadata": {},
   "source": [
    "## Step 3: Import Libraries\n",
    "\n",
    "Import all necessary libraries for video capture and hand detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbb16b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf87c5",
   "metadata": {},
   "source": [
    "## Step 4: Visualization Function\n",
    "\n",
    "This function draws the hand landmarks and skeleton on the image.\n",
    "\n",
    "**What it does:**\n",
    "- Loops through each detected hand\n",
    "- Draws 21 landmark points connected by lines\n",
    "- Labels each hand as \"Left\" or \"Right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b8fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization and gesture detection functions defined!\n",
      "Gestures detected: Thumbs Up ðŸ‘, Thumbs Down ðŸ‘Ž\n"
     ]
    }
   ],
   "source": [
    "# Constants for drawing\n",
    "MARGIN = 10  # pixels above hand to place label\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 2\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54)  # Vibrant green (BGR format)\n",
    "\n",
    "# Hand landmark indices\n",
    "WRIST = 0\n",
    "THUMB_CMC = 1\n",
    "THUMB_MCP = 2\n",
    "THUMB_IP = 3\n",
    "THUMB_TIP = 4\n",
    "INDEX_MCP = 5\n",
    "INDEX_TIP = 8\n",
    "MIDDLE_MCP = 9\n",
    "MIDDLE_TIP = 12\n",
    "RING_MCP = 13\n",
    "RING_TIP = 16\n",
    "PINKY_MCP = 17\n",
    "PINKY_TIP = 20\n",
    "\n",
    "def detect_thumbs_gesture(hand_landmarks):\n",
    "    \"\"\"\n",
    "    Detects thumbs up or thumbs down gesture.\n",
    "    \n",
    "    Logic:\n",
    "    - Thumbs UP: Thumb tip is ABOVE (lower y) the thumb MCP, and other fingers are curled\n",
    "    - Thumbs DOWN: Thumb tip is BELOW (higher y) the thumb MCP, and other fingers are curled\n",
    "    - Fingers are \"curled\" when their tips are below their MCP joints (higher y value)\n",
    "    \n",
    "    Args:\n",
    "        hand_landmarks: List of 21 hand landmarks from MediaPipe\n",
    "    \n",
    "    Returns:\n",
    "        \"thumbs_up\", \"thumbs_down\", or None\n",
    "    \"\"\"\n",
    "    # Get relevant landmarks\n",
    "    thumb_tip = hand_landmarks[THUMB_TIP]\n",
    "    thumb_mcp = hand_landmarks[THUMB_MCP]\n",
    "    thumb_ip = hand_landmarks[THUMB_IP]\n",
    "    wrist = hand_landmarks[WRIST]\n",
    "    \n",
    "    index_tip = hand_landmarks[INDEX_TIP]\n",
    "    index_mcp = hand_landmarks[INDEX_MCP]\n",
    "    middle_tip = hand_landmarks[MIDDLE_TIP]\n",
    "    middle_mcp = hand_landmarks[MIDDLE_MCP]\n",
    "    ring_tip = hand_landmarks[RING_TIP]\n",
    "    ring_mcp = hand_landmarks[RING_MCP]\n",
    "    pinky_tip = hand_landmarks[PINKY_TIP]\n",
    "    pinky_mcp = hand_landmarks[PINKY_MCP]\n",
    "    \n",
    "    # Check if fingers are curled (tips below MCPs - higher y value means lower on screen)\n",
    "    index_curled = index_tip.y > index_mcp.y\n",
    "    middle_curled = middle_tip.y > middle_mcp.y\n",
    "    ring_curled = ring_tip.y > ring_mcp.y\n",
    "    pinky_curled = pinky_tip.y > pinky_mcp.y\n",
    "    \n",
    "    # All four fingers must be curled for a thumbs gesture\n",
    "    fingers_curled = index_curled and middle_curled and ring_curled and pinky_curled\n",
    "    \n",
    "    if not fingers_curled:\n",
    "        return None\n",
    "    \n",
    "    # Calculate thumb direction\n",
    "    # Thumb tip vs thumb MCP (base of thumb)\n",
    "    thumb_vertical_diff = thumb_mcp.y - thumb_tip.y  # Positive = thumb pointing up\n",
    "    \n",
    "    # Threshold for detection (normalized coordinates, so ~0.1 is significant)\n",
    "    threshold = 0.07\n",
    "    \n",
    "    # Also check thumb is extended (tip is far from IP joint)\n",
    "    thumb_extended = abs(thumb_tip.y - thumb_ip.y) > 0.02\n",
    "    \n",
    "    if thumb_vertical_diff > threshold and thumb_extended:\n",
    "        return \"thumbs_up\"\n",
    "    elif thumb_vertical_diff < -threshold and thumb_extended:\n",
    "        return \"thumbs_down\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    \"\"\"\n",
    "    Draws hand landmarks, handedness labels, and gesture detection on an image.\n",
    "    \n",
    "    Args:\n",
    "        rgb_image: Input image as NumPy array (RGB format)\n",
    "        detection_result: MediaPipe HandLandmarkerResult object\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (annotated_image, list of detected gestures)\n",
    "    \"\"\"\n",
    "    # Get lists of landmarks and handedness from detection result\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    handedness_list = detection_result.handedness\n",
    "    \n",
    "    # Create a copy so we don't modify the original\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "    \n",
    "    # Store detected gestures\n",
    "    gestures = []\n",
    "\n",
    "    # Loop through each detected hand\n",
    "    for idx in range(len(hand_landmarks_list)):\n",
    "        hand_landmarks = hand_landmarks_list[idx]  # 21 landmarks for this hand\n",
    "        handedness = handedness_list[idx]  # Left or Right classification\n",
    "\n",
    "        # Detect thumbs gesture\n",
    "        gesture = detect_thumbs_gesture(hand_landmarks)\n",
    "        \n",
    "        # Get corrected handedness (for mirrored display)\n",
    "        original_label = handedness[0].category_name\n",
    "        corrected_label = \"Right\" if original_label == \"Left\" else \"Left\"\n",
    "        \n",
    "        gestures.append({\n",
    "            \"hand\": corrected_label,\n",
    "            \"gesture\": gesture\n",
    "        })\n",
    "\n",
    "        # Convert landmarks to protobuf format (required by MediaPipe drawing utils)\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(\n",
    "                x=landmark.x, \n",
    "                y=landmark.y, \n",
    "                z=landmark.z\n",
    "            ) for landmark in hand_landmarks\n",
    "        ])\n",
    "        \n",
    "        # Draw the hand skeleton (dots + connecting lines)\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,\n",
    "            hand_landmarks_proto,\n",
    "            solutions.hands.HAND_CONNECTIONS,  # Defines which landmarks to connect\n",
    "            solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "            solutions.drawing_styles.get_default_hand_connections_style()\n",
    "        )\n",
    "\n",
    "        # Calculate position for the handedness label (top-left of hand)\n",
    "        height, width, _ = annotated_image.shape\n",
    "        x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "        y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "        \n",
    "        # Convert normalized coordinates (0-1) to pixel coordinates\n",
    "        text_x = int(min(x_coordinates) * width)\n",
    "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "        \n",
    "        # Build label with gesture\n",
    "        if gesture == \"thumbs_up\":\n",
    "            label = f\"{corrected_label} ðŸ‘ THUMBS UP!\"\n",
    "            color = (0, 255, 0)  # Green for thumbs up\n",
    "        elif gesture == \"thumbs_down\":\n",
    "            label = f\"{corrected_label} ðŸ‘Ž THUMBS DOWN!\"\n",
    "            color = (255, 0, 0)  # Red for thumbs down\n",
    "        else:\n",
    "            label = corrected_label\n",
    "            color = HANDEDNESS_TEXT_COLOR\n",
    "        \n",
    "        cv2.putText(\n",
    "            annotated_image,\n",
    "            label,\n",
    "            (text_x, text_y),\n",
    "            cv2.FONT_HERSHEY_DUPLEX,\n",
    "            FONT_SIZE,\n",
    "            color,\n",
    "            FONT_THICKNESS,\n",
    "            cv2.LINE_AA  # Anti-aliased (smooth) text\n",
    "        )\n",
    "\n",
    "    return annotated_image, gestures\n",
    "\n",
    "print(\"Visualization and gesture detection functions defined!\")\n",
    "print(\"Gestures detected: Thumbs Up ðŸ‘, Thumbs Down ðŸ‘Ž\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bd6bc3",
   "metadata": {},
   "source": [
    "## Step 5: Initialize the Hand Detector\n",
    "\n",
    "Create the HandLandmarker detector configured for VIDEO mode.\n",
    "\n",
    "**VIDEO mode vs IMAGE mode:**\n",
    "- VIDEO mode uses tracking between frames (faster, smoother)\n",
    "- Requires a timestamp for each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "020640b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hand detector initialized!\n",
      "- Running mode: VIDEO\n",
      "- Max hands: 2\n",
      "- Detection confidence: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1768382607.221729    5709 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1768382607.223716    6421 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) Graphics (ADL GT2)\n",
      "W0000 00:00:1768382607.235733    6432 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1768382607.248716    6425 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Configure the detector\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    running_mode=vision.RunningMode.VIDEO,  # Optimized for video frames\n",
    "    num_hands=2,  # Detect up to 2 hands\n",
    "    min_hand_detection_confidence=0.5,  # Minimum confidence to detect a hand\n",
    "    min_hand_presence_confidence=0.5,   # Minimum confidence hand is still present\n",
    "    min_tracking_confidence=0.5         # Minimum confidence for tracking between frames\n",
    ")\n",
    "\n",
    "# Create the detector\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "print(\"Hand detector initialized!\")\n",
    "print(f\"- Running mode: VIDEO\")\n",
    "print(f\"- Max hands: 2\")\n",
    "print(f\"- Detection confidence: 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f384e7d",
   "metadata": {},
   "source": [
    "## Step 6: Run Live Detection with Gesture Recognition! ðŸŽ¥\n",
    "\n",
    "This cell opens your webcam and runs hand detection with **thumbs up/down gesture recognition**.\n",
    "\n",
    "**Gesture Detection Logic:**\n",
    "- **Thumbs UP ðŸ‘**: Thumb tip is ABOVE the thumb base, all other fingers are curled (closed fist)\n",
    "- **Thumbs DOWN ðŸ‘Ž**: Thumb tip is BELOW the thumb base, all other fingers are curled\n",
    "\n",
    "**Controls:**\n",
    "- Press **'q'** to quit and close the window\n",
    "- Press **'b'** to toggle Blue Glove Mode\n",
    "\n",
    "**What you'll see:**\n",
    "- Live video feed with hand landmarks drawn\n",
    "- FPS (frames per second) displayed\n",
    "- \"Left\" or \"Right\" labels on each detected hand\n",
    "- **Green \"THUMBS UP!\"** when showing thumbs up\n",
    "- **Red \"THUMBS DOWN!\"** when showing thumbs down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera opened successfully!\n",
      "Press 'q' to quit, 'b' to toggle Blue Glove Mode\n",
      "----------------------------------------\n",
      "Show THUMBS UP ðŸ‘ or THUMBS DOWN ðŸ‘Ž to the camera!\n",
      "----------------------------------------\n",
      "Blue Glove Mode: OFF\n",
      "Blue Glove Mode: ON\n"
     ]
    }
   ],
   "source": [
    "# Open the webcam (0 = default camera, try 1 or 2 if you have multiple cameras)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"ERROR: Could not open camera!\")\n",
    "    print(\"Try changing VideoCapture(0) to VideoCapture(1)\")\n",
    "else:\n",
    "    print(\"Camera opened successfully!\")\n",
    "    print(\"Press 'q' to quit, 'b' to toggle Blue Glove Mode\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Show THUMBS UP ðŸ‘ or THUMBS DOWN ðŸ‘Ž to the camera!\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Variables for FPS calculation\n",
    "prev_time = 0\n",
    "fps = 0\n",
    "\n",
    "# Toggle for blue glove mode (swap R and B channels)\n",
    "BLUE_GLOVE_MODE = True  # Set to True when wearing blue gloves\n",
    "\n",
    "# Gesture counters for display\n",
    "gesture_counts = {\"thumbs_up\": 0, \"thumbs_down\": 0}\n",
    "\n",
    "# Main loop - runs until 'q' is pressed\n",
    "while cap.isOpened():\n",
    "    # Step 1: Read a frame from the webcam\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    if not success:\n",
    "        print(\"Failed to read frame from camera\")\n",
    "        break\n",
    "    \n",
    "    # Step 2: Flip the frame horizontally (mirror effect - more intuitive)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Step 3: Convert BGR (OpenCV format) to RGB (MediaPipe format)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Step 3.5: BLUE GLOVE MODE - Swap R and B channels for detection\n",
    "    if BLUE_GLOVE_MODE:\n",
    "        frame_for_detection = np.ascontiguousarray(frame_rgb[:, :, [2, 1, 0]])\n",
    "    else:\n",
    "        frame_for_detection = frame_rgb\n",
    "    \n",
    "    # Step 4: Create MediaPipe Image object\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_for_detection)\n",
    "    \n",
    "    # Step 5: Get current timestamp in milliseconds\n",
    "    timestamp_ms = int(time.time() * 1000)\n",
    "    \n",
    "    # Step 6: Run hand detection\n",
    "    detection_result = detector.detect_for_video(mp_image, timestamp_ms)\n",
    "    \n",
    "    # Step 7: Draw landmarks and detect gestures\n",
    "    annotated_frame, gestures = draw_landmarks_on_image(frame_rgb, detection_result)\n",
    "    \n",
    "    # Count gestures\n",
    "    for g in gestures:\n",
    "        if g[\"gesture\"] == \"thumbs_up\":\n",
    "            gesture_counts[\"thumbs_up\"] += 1\n",
    "        elif g[\"gesture\"] == \"thumbs_down\":\n",
    "            gesture_counts[\"thumbs_down\"] += 1\n",
    "    \n",
    "    # Step 8: Convert back to BGR for OpenCV display\n",
    "    annotated_frame_bgr = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Step 9: Calculate and display FPS\n",
    "    current_time = time.time()\n",
    "    fps = 1 / (current_time - prev_time) if prev_time > 0 else 0\n",
    "    prev_time = current_time\n",
    "    \n",
    "    # Draw FPS on the frame\n",
    "    cv2.putText(\n",
    "        annotated_frame_bgr,\n",
    "        f\"FPS: {fps:.1f}\",\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 255, 0),\n",
    "        2,\n",
    "        cv2.LINE_AA\n",
    "    )\n",
    "    \n",
    "    # Draw number of hands detected\n",
    "    num_hands = len(detection_result.hand_landmarks)\n",
    "    cv2.putText(\n",
    "        annotated_frame_bgr,\n",
    "        f\"Hands: {num_hands}\",\n",
    "        (10, 70),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 255, 0),\n",
    "        2,\n",
    "        cv2.LINE_AA\n",
    "    )\n",
    "    \n",
    "    # Show blue glove mode status\n",
    "    mode_text = \"Blue Glove: ON\" if BLUE_GLOVE_MODE else \"Blue Glove: OFF\"\n",
    "    cv2.putText(\n",
    "        annotated_frame_bgr,\n",
    "        mode_text,\n",
    "        (10, 110),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.7,\n",
    "        (255, 165, 0) if BLUE_GLOVE_MODE else (128, 128, 128),\n",
    "        2,\n",
    "        cv2.LINE_AA\n",
    "    )\n",
    "    \n",
    "    # Show gesture instructions\n",
    "    cv2.putText(\n",
    "        annotated_frame_bgr,\n",
    "        \"Show THUMBS UP or DOWN!\",\n",
    "        (10, 150),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.7,\n",
    "        (255, 255, 255),\n",
    "        2,\n",
    "        cv2.LINE_AA\n",
    "    )\n",
    "    \n",
    "    # Display current gesture detection in big text at bottom\n",
    "    current_gesture = None\n",
    "    for g in gestures:\n",
    "        if g[\"gesture\"]:\n",
    "            current_gesture = g[\"gesture\"]\n",
    "            break\n",
    "    \n",
    "    if current_gesture == \"thumbs_up\":\n",
    "        cv2.putText(\n",
    "            annotated_frame_bgr,\n",
    "            \"THUMBS UP!\",\n",
    "            (frame.shape[1] // 2 - 150, frame.shape[0] - 50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1.5,\n",
    "            (0, 255, 0),\n",
    "            3,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "    elif current_gesture == \"thumbs_down\":\n",
    "        cv2.putText(\n",
    "            annotated_frame_bgr,\n",
    "            \"THUMBS DOWN!\",\n",
    "            (frame.shape[1] // 2 - 180, frame.shape[0] - 50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1.5,\n",
    "            (0, 0, 255),\n",
    "            3,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "    \n",
    "    # Step 10: Display the frame in a window\n",
    "    cv2.imshow('Hand Gesture Detection - Press Q to Quit, B to Toggle Blue Glove Mode', annotated_frame_bgr)\n",
    "    \n",
    "    # Step 11: Check for key presses\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        print(\"\\nQuitting...\")\n",
    "        break\n",
    "    elif key == ord('b'):\n",
    "        BLUE_GLOVE_MODE = not BLUE_GLOVE_MODE\n",
    "        status = \"ON\" if BLUE_GLOVE_MODE else \"OFF\"\n",
    "        print(f\"Blue Glove Mode: {status}\")\n",
    "\n",
    "# Cleanup: Release the camera and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Camera released and windows closed!\")\n",
    "print(f\"\\nGesture Summary:\")\n",
    "print(f\"  Thumbs Up detected: {gesture_counts['thumbs_up']} frames\")\n",
    "print(f\"  Thumbs Down detected: {gesture_counts['thumbs_down']} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a597fcb6",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| Camera not found | Change `VideoCapture(0)` to `VideoCapture(1)` or `VideoCapture(2)` |\n",
    "| Window doesn't close | Make sure to press 'q' key, not the X button |\n",
    "| Very slow/laggy | Reduce frame size or check CPU/GPU usage |\n",
    "| Model not found | Run the model download cell first |\n",
    "| Hands not detected | Ensure good lighting and hands are clearly visible |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367e798",
   "metadata": {},
   "source": [
    "## Optional: Reduce Frame Size for Better Performance\n",
    "\n",
    "If the detection is slow, you can reduce the frame resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e97848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution set to: 640x480\n"
     ]
    }
   ],
   "source": [
    "# Run this cell if you want to use a lower resolution for better performance\n",
    "# Then run the main detection cell again\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set resolution to 640x480 (lower = faster)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "print(f\"Resolution set to: {int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))}x{int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))}\")\n",
    "\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
