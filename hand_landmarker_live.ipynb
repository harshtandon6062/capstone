{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11514d08",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "Make sure MediaPipe is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b9225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae075c5",
   "metadata": {},
   "source": [
    "## Step 2: Download the Model\n",
    "\n",
    "Download the hand landmarker model if it doesn't exist already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39d91bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Only download if the model doesn't exist\n",
    "if not os.path.exists('hand_landmarker.task'):\n",
    "    print(\"Downloading model...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        'https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task',\n",
    "        'hand_landmarker.task'\n",
    "    )\n",
    "    print(\"Model downloaded successfully!\")\n",
    "else:\n",
    "    print(\"Model already exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb9146",
   "metadata": {},
   "source": [
    "## Step 3: Import Libraries\n",
    "\n",
    "Import all necessary libraries for video capture and hand detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb16b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf87c5",
   "metadata": {},
   "source": [
    "## Step 4: Visualization Function\n",
    "\n",
    "This function draws the hand landmarks and skeleton on the image.\n",
    "\n",
    "**What it does:**\n",
    "- Loops through each detected hand\n",
    "- Draws 21 landmark points connected by lines\n",
    "- Labels each hand as \"Left\" or \"Right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "853b8fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization function defined!\n"
     ]
    }
   ],
   "source": [
    "# Constants for drawing\n",
    "MARGIN = 10  # pixels above hand to place label\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 2\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54)  # Vibrant green (BGR format)\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    \"\"\"\n",
    "    Draws hand landmarks and handedness labels on an image.\n",
    "    \n",
    "    Args:\n",
    "        rgb_image: Input image as NumPy array (RGB format)\n",
    "        detection_result: MediaPipe HandLandmarkerResult object\n",
    "    \n",
    "    Returns:\n",
    "        Image with landmarks drawn on it\n",
    "    \"\"\"\n",
    "    # Get lists of landmarks and handedness from detection result\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    handedness_list = detection_result.handedness\n",
    "    \n",
    "    # Create a copy so we don't modify the original\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through each detected hand\n",
    "    for idx in range(len(hand_landmarks_list)):\n",
    "        hand_landmarks = hand_landmarks_list[idx]  # 21 landmarks for this hand\n",
    "        handedness = handedness_list[idx]  # Left or Right classification\n",
    "\n",
    "        # Convert landmarks to protobuf format (required by MediaPipe drawing utils)\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(\n",
    "                x=landmark.x, \n",
    "                y=landmark.y, \n",
    "                z=landmark.z\n",
    "            ) for landmark in hand_landmarks\n",
    "        ])\n",
    "        \n",
    "        # Draw the hand skeleton (dots + connecting lines)\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,\n",
    "            hand_landmarks_proto,\n",
    "            solutions.hands.HAND_CONNECTIONS,  # Defines which landmarks to connect\n",
    "            solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "            solutions.drawing_styles.get_default_hand_connections_style()\n",
    "        )\n",
    "\n",
    "        # Calculate position for the handedness label (top-left of hand)\n",
    "        height, width, _ = annotated_image.shape\n",
    "        x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "        y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "        \n",
    "        # Convert normalized coordinates (0-1) to pixel coordinates\n",
    "        text_x = int(min(x_coordinates) * width)\n",
    "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "        # Draw \"Left\" or \"Right\" label\n",
    "        # Since we flip the image horizontally for mirror effect, we need to \n",
    "        # swap the handedness labels (Left becomes Right, Right becomes Left)\n",
    "        original_label = handedness[0].category_name\n",
    "        corrected_label = \"Right\" if original_label == \"Left\" else \"Left\"\n",
    "        \n",
    "        cv2.putText(\n",
    "            annotated_image,\n",
    "            f\"{corrected_label}\",  # Corrected label for mirrored display\n",
    "            (text_x, text_y),\n",
    "            cv2.FONT_HERSHEY_DUPLEX,\n",
    "            FONT_SIZE,\n",
    "            HANDEDNESS_TEXT_COLOR,\n",
    "            FONT_THICKNESS,\n",
    "            cv2.LINE_AA  # Anti-aliased (smooth) text\n",
    "        )\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "print(\"Visualization function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bd6bc3",
   "metadata": {},
   "source": [
    "## Step 5: Initialize the Hand Detector\n",
    "\n",
    "Create the HandLandmarker detector configured for VIDEO mode.\n",
    "\n",
    "**VIDEO mode vs IMAGE mode:**\n",
    "- VIDEO mode uses tracking between frames (faster, smoother)\n",
    "- Requires a timestamp for each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020640b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hand detector initialized!\n",
      "- Running mode: VIDEO\n",
      "- Max hands: 2\n",
      "- Detection confidence: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1768317182.778465   11698 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1768317182.780812   11801 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) Graphics (ADL GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1768317182.789672   11813 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1768317182.804196   11803 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Configure the detector\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    running_mode=vision.RunningMode.VIDEO,  # Optimized for video frames\n",
    "    num_hands=2,  # Detect up to 2 hands\n",
    "    min_hand_detection_confidence=0.5,  # Minimum confidence to detect a hand\n",
    "    min_hand_presence_confidence=0.5,   # Minimum confidence hand is still present\n",
    "    min_tracking_confidence=0.5         # Minimum confidence for tracking between frames\n",
    ")\n",
    "\n",
    "# Create the detector\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "print(\"Hand detector initialized!\")\n",
    "print(f\"- Running mode: VIDEO\")\n",
    "print(f\"- Max hands: 2\")\n",
    "print(f\"- Detection confidence: 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f384e7d",
   "metadata": {},
   "source": [
    "## Step 6: Run Live Detection! ðŸŽ¥\n",
    "\n",
    "This cell opens your webcam and runs hand detection in real-time.\n",
    "\n",
    "**Controls:**\n",
    "- Press **'q'** to quit and close the window\n",
    "\n",
    "**What you'll see:**\n",
    "- Live video feed with hand landmarks drawn\n",
    "- FPS (frames per second) displayed in top-left corner\n",
    "- \"Left\" or \"Right\" labels on each detected hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caa8db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera opened successfully!\n",
      "Press 'q' to quit...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh-tandon/capstone/venv/lib/python3.12/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quitting...\n",
      "Camera released and windows closed!\n"
     ]
    }
   ],
   "source": [
    "# Open the webcam (0 = default camera, try 1 or 2 if you have multiple cameras)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"ERROR: Could not open camera!\")\n",
    "    print(\"Try changing VideoCapture(0) to VideoCapture(1)\")\n",
    "else:\n",
    "    print(\"Camera opened successfully!\")\n",
    "    print(\"Press 'q' to quit...\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Variables for FPS calculation\n",
    "prev_time = 0\n",
    "fps = 0\n",
    "\n",
    "# Main loop - runs until 'q' is pressed\n",
    "while cap.isOpened():\n",
    "    # Step 1: Read a frame from the webcam\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    if not success:\n",
    "        print(\"Failed to read frame from camera\")\n",
    "        break\n",
    "    \n",
    "    # Step 2: Flip the frame horizontally (mirror effect - more intuitive)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Step 3: Convert BGR (OpenCV format) to RGB (MediaPipe format)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Step 4: Create MediaPipe Image object\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "    \n",
    "    # Step 5: Get current timestamp in milliseconds (required for VIDEO mode)\n",
    "    timestamp_ms = int(time.time() * 1000)\n",
    "    \n",
    "    # Step 6: Run hand detection\n",
    "    detection_result = detector.detect_for_video(mp_image, timestamp_ms)\n",
    "    \n",
    "    # Step 7: Draw landmarks on the frame\n",
    "    annotated_frame = draw_landmarks_on_image(frame_rgb, detection_result)\n",
    "    \n",
    "    # Step 8: Convert back to BGR for OpenCV display\n",
    "    annotated_frame_bgr = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Step 9: Calculate and display FPS\n",
    "    current_time = time.time()\n",
    "    fps = 1 / (current_time - prev_time) if prev_time > 0 else 0\n",
    "    prev_time = current_time\n",
    "    \n",
    "    # Draw FPS on the frame\n",
    "    cv2.putText(\n",
    "        annotated_frame_bgr,\n",
    "        f\"FPS: {fps:.1f}\",\n",
    "        (10, 30),  # Position: top-left corner\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,  # Font size\n",
    "        (0, 255, 0),  # Green color (BGR)\n",
    "        2,  # Thickness\n",
    "        cv2.LINE_AA\n",
    "    )\n",
    "    \n",
    "    # Draw number of hands detected\n",
    "    num_hands = len(detection_result.hand_landmarks)\n",
    "    cv2.putText(\n",
    "        annotated_frame_bgr,\n",
    "        f\"Hands: {num_hands}\",\n",
    "        (10, 70),  # Position: below FPS\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 255, 0),\n",
    "        2,\n",
    "        cv2.LINE_AA\n",
    "    )\n",
    "    \n",
    "    # Step 10: Display the frame in a window\n",
    "    cv2.imshow('Hand Landmark Detection - Press Q to Quit', annotated_frame_bgr)\n",
    "    \n",
    "    # Step 11: Check for 'q' key press to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"\\nQuitting...\")\n",
    "        break\n",
    "\n",
    "# Cleanup: Release the camera and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Camera released and windows closed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a597fcb6",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| Camera not found | Change `VideoCapture(0)` to `VideoCapture(1)` or `VideoCapture(2)` |\n",
    "| Window doesn't close | Make sure to press 'q' key, not the X button |\n",
    "| Very slow/laggy | Reduce frame size or check CPU/GPU usage |\n",
    "| Model not found | Run the model download cell first |\n",
    "| Hands not detected | Ensure good lighting and hands are clearly visible |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367e798",
   "metadata": {},
   "source": [
    "## Optional: Reduce Frame Size for Better Performance\n",
    "\n",
    "If the detection is slow, you can reduce the frame resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5e97848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution set to: 640x480\n"
     ]
    }
   ],
   "source": [
    "# Run this cell if you want to use a lower resolution for better performance\n",
    "# Then run the main detection cell again\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set resolution to 640x480 (lower = faster)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "print(f\"Resolution set to: {int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))}x{int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))}\")\n",
    "\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
